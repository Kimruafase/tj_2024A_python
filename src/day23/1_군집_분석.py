# day23 > 1_군집_분석.py

# 비지도 학습 : 타깃(종속)값이 주어지지 않은 상태에서 학습 수행 vs 지도 학습 : 회귀 분석, 분류 분석

# 군집 분석 : 데이터 포인트(군집)를 사전에 정의된 군집계수(K)로 그룹화하여 유사한 데이터들을 군집에 배치하여 새로운 데이터의 군집 예측
    # 특징 : 1. 비지도 학습 2. 사전에 클러스터 수 필요 (K) 3. K-평균 알고리즘 필요
# 군집화 / 클러스터화 : 학습을 수행하여 데이터 간의 관계를 붆석하고 이에 따른 유사한 데이터들을 군집으로 구성하는 작업
# K평균 알고리즘 : K개의 클러스터를 구성하는 알고리즘

# 최적의 클러스터 수 찾기 : 1. 엘보 방법 2. 실루엣 방법


import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# [1] 데이터 수집 # 종속변수 X
data = {
    'weight': [110, 160, 130, 320, 370, 300, 55, 65, 60, 210, 220, 200, 90, 80, 100, 190, 180, 170, 100, 90,
               140, 280, 320, 130, 200, 140, 250, 150, 70, 80, 200, 300, 220, 140, 180, 230, 220, 250],
    'sweetness': [6.2, 7.2, 6.8, 8.1, 8.6, 8.1, 5.2, 5.7, 6.1, 7.2, 7.6, 6.7, 7.3, 6.9, 7.3, 7.5, 7.4, 7.3, 7.0, 6.8,
                  6.9, 8.0, 8.1, 6.7, 7.0, 6.6, 7.8, 7.1, 6.7, 6.5, 7.0, 7.6, 7.3, 7.0, 7.2, 7.5, 7.4, 7.7]
}

df = pd.DataFrame(data)

# [2] K-평균 군집 분석 모델
    # 군집 분석 모델 객체 생성
model = KMeans()

    # 모델에 데이터 피팅
model.fit(df)       # 비지도 학습이기 떄문에 종속 변수가 없다.

# [3] 클러스터 / 군집 / 중심지 확인, .cluster_centers_
# print(model.cluster_centers_)
"""
[[188.57142857   7.15714286]
 [ 68.33333333   6.18333333]
 [304.           7.98      ]
 [141.42857143   6.9       ]
 [220.           7.4       ]
 [370.           8.6       ]
 [ 98.           6.92      ]
 [250.           7.75      ]]
"""

# [4] 군집 결과를 확인, .labels
# print(model.labels_)    # 0 ~ 2 [1 0 1 2 2 2 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 2 2 1 0 1 0 1 1 1 0 2 0 1 0 0 0 0]

# [5] 결과를 DataFrame 에 추가
df["cluster"] = model.labels_
# print(df)
"""
    weight  sweetness  cluster
0      110        6.2        1
1      160        7.2        0
2      130        6.8        1
3      320        8.1        2
4      370        8.6        2
5      300        8.1        2
6       55        5.2        1
7       65        5.7        1
8       60        6.1        1
9      210        7.2        0
10     220        7.6        0
11     200        6.7        0
12      90        7.3        1
13      80        6.9        1
14     100        7.3        1
15     190        7.5        0
16     180        7.4        0
17     170        7.3        0
18     100        7.0        1
19      90        6.8        1
20     140        6.9        1
21     280        8.0        2
22     320        8.1        2
23     130        6.7        1
24     200        7.0        0
25     140        6.6        1
26     250        7.8        0
27     150        7.1        1
28      70        6.7        1
29      80        6.5        1
30     200        7.0        0
31     300        7.6        2
32     220        7.3        0
33     140        7.0        1
34     180        7.2        0
35     230        7.5        0
36     220        7.4        0
37     250        7.7        0
"""

# [6] 새로운 데이터로 군집 예측

new_data = {"weight" : [110], "sweetness" : [7]}    # 새로운 과일의 무게와 당도 1개

new_df = pd.DataFrame(new_data)

# [7] 예측하기
c_pred = model.predict(new_df)
# print(c_pred)       # [1]


# [8] 시각화
# plt.scatter(df["weight"], df["sweetness"], c=df["cluster"], marker="o")
# plt.scatter(new_df["weight"], new_df["sweetness"], marker = "^")
# plt.show()

# [9] 개선

# 개선 1 : 무게와 당도의 범위가 크다. 스케일 차이가 크다. -> 무게는 100 단위, 당도는 1 단위이기 때문에 차이가 크다.
# 데이터 분석에서 스케일 차이가 크면 특정 속성의 비중을 많이 차지하게 된다.
# 스케일 표준화 : 알고리즘의 성능을 개선하여 좀 더 좋은 학습을 하기 위해서 필요한 작업 -> StandardScaler
    # 스케일 객체 생성
scaler = StandardScaler()

    # 무게와 당도의 스케일 맞추기 -> 데이터 표준화
    # 여러개의 데이터 평균을 0으로 맞추고 표쥰 편차를 1이 되도록 변환하는 과정
    # 무게 100 단위, 당도 1단위 -> 특정 알고리즘(K-평균)에서 무게에 더 큰 비중을 갖는다.
    # 표준 편차가 크다 -> 데이터가 평균으로부터의 차이가 크다 -> 데이터가 매우 다양하다.
    # 표준 편차가 작다 -> 데이터가 평균으로부터의 차이가 작다 -> 데이터가 비슷하다.
scaled_data = scaler.fit_transform(df[["weight", "sweetness"]])
# print(scaled_data)
"""
[[-0.81219782 -1.40395189]
 [-0.19982645  0.09886985]
 [-0.56724927 -0.50225884]
 [ 1.75976195  1.45140942]
 [ 2.37213333  2.20282029]
 [ 1.5148134   1.45140942]
 [-1.48580634 -2.90677363]
 [-1.36333206 -2.15536276]
 [-1.4245692  -1.55423406]
 [ 0.41254493  0.09886985]
 [ 0.5350192   0.69999855]
 [ 0.29007065 -0.65254102]
 [-1.05714637  0.24915203]
 [-1.17962065 -0.35197667]
 [-0.9346721   0.24915203]
 [ 0.16759638  0.54971637]
 [ 0.0451221   0.3994342 ]
 [-0.07735217  0.24915203]
 [-0.9346721  -0.2016945 ]
 [-1.05714637 -0.50225884]
 [-0.444775   -0.35197667]
 [ 1.26986485  1.30112724]
 [ 1.75976195  1.45140942]
 [-0.56724927 -0.65254102]
 [ 0.29007065 -0.2016945 ]
 [-0.444775   -0.80282319]
 [ 0.90244203  1.0005629 ]
 [-0.32230072 -0.05141232]
 [-1.30209492 -0.65254102]
 [-1.17962065 -0.95310537]
 [ 0.29007065 -0.2016945 ]
 [ 1.5148134   0.69999855]
 [ 0.5350192   0.24915203]
 [-0.444775   -0.2016945 ]
 [ 0.0451221   0.09886985]
 [ 0.65749348  0.54971637]
 [ 0.5350192   0.3994342 ]
 [ 0.90244203  0.85028072]]
"""

    # 스케일된 데이터로 모델 학습
model2 = KMeans(n_clusters=3)

    # 스케일된 데이터로 fit
model2.fit(scaled_data)

    # 클러스터 결과를 DataFrame에 대입
df["cluster"] = model2.labels_
df["scaled_weight"] = scaled_data[:, 0] # 첫번째 열의 모든 행 추출(스케일된 무게)
df["scaled_sweetness"] = scaled_data[:, 1] # 두번째 열의 모든 행 추출(스케일된 당도)

    # 새로운 데이터 예측
new_scaled_data = scaler.fit_transform(new_df[["weight","sweetness"]])
c_pred2 = model2.predict(new_scaled_data)

    # 시각화
# plt.scatter(df["scaled_weight"], df["scaled_sweetness"], c=df["cluster"], marker="o")
# plt.scatter(new_scaled_data[:, 0], new_scaled_data[:, 1], marker="^")
# plt.show()

# 개선 2 : 최적의 K (클러스터 수) 찾기
    # 그래프에서 SSE(왜곡)의 변화가 급격히 줄어드는 지점ㅁ을 찾는다. 그 지점이 최적의 클러스터 수이다.
error = []  # 오차(error)를 저장하는 리스트

for c in range(1 , 11) :             # 1 ~ 11인 클러스터 수 테스트
    model3 = KMeans(n_clusters=c)    #
    model3.fit(scaled_data)
    print(model3.inertia_)           # (데이터들 간의 거리 차이의 제곱값의 합)총 제곱 오차(SSE) 계산하고 반환한다.
    error.append(model3.inertia_)    # SSE를 리스트에 대입

# 총 클러스터 1부터 10개까지의 모델 10개 SSE(오차)를 리스트에 저장
print(error)
# [75.99999999999999, 32.312795640570236, 18.752237657305717, 10.656737745099216, 8.495498201687349, 7.265179551853112, 5.671659899501681, 4.978318868764475, 3.6350038592979494, 3.356823106262362]

# 오차 시각화
plt.plot(error, marker="o")
# plt.show()

"""
    ex_
    cluster = [1, 2, 3, 4, 5]
    sse = [400, 200, 100, 80, 75]   # 급격하게 변화가 줄어드는 지점 (cluster 수가 3 -> 4로 변화할 때)
             200  100   20   5
"""

    # 최적의 K 값 확인 후 이용한 재 모델링
model4 = KMeans(n_clusters=2)

model4.fit(scaled_data)
